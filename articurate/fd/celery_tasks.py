from __future__ import absolute_import

import nltk
import re
import urllib2
import opml
import feedparser
import boto
import simplejson
from time import mktime
from datetime import datetime, date, time, timedelta
from xml.etree.ElementTree import Element, SubElement, Comment
from xml.dom import minidom
from xml.etree import ElementTree
import logging
import os
from boto.s3.connection import S3Connection
from boto.s3.key import Key
import requests
from urlparse import urlparse,urlunparse
from readability.readability import Document
import pickle

from articurate.metrics import metrics
from articurate.pymotherlode.api import *

from articurate.utils.config import *

from celery import chord, group
from articurate.celery import celery
import articurate
from articurate.metrics import metrics
from celery import current_task
from celery import task
from celery.utils.log import get_task_logger

from articurate.fd.fd import *


SERVER_URL = "http://localhost:9999/"
logger = get_task_logger(__name__)

#@celery.task
#def runFD():
#	try:
#		fd.startFD()
#		return 'True'
#	except:
#		return 'False'

@metrics.track_by("tech")
def saveDataToRedis(interval):
    # generates a unique ID based on time
    stringID = genStringID()
    generated_on = datetime.utcnow()

    datapack = {'interval': interval, 'stringID': stringID, 'generated_on': generated_on}

    return datapack



"""dumps articles found in [currentTime-interval:currentTime] minutes"""
@celery.task
def run_fd(interval):
    print "run_fd: starting"

    # generates a unique ID based on time
    #stringID = genStringID()
    #generated_on = datetime.utcnow()

    datapack = saveDataToRedis(interval)

    stringID = datapack['stringID']

    # set up python logger
    logger = logging.getLogger("["+stringID+"]")
    logger.setLevel(logging.INFO)

    # get RSS sources from the web
    if config['db.coldStart']:
        rss_sources_json = getRSSSources()
        rss_sources = rss_sources_json['rss']
    #Example of getting a function output by key
    else:
        rss_sources_json = getMetricByKey("articurate.fd.fd.getRSSSources", "tech")
        rss_sources = rss_sources_json['rss']

    try:
        all_rss_sources = [source for count, source in enumerate(rss_sources)]

        result = chord(parse_source_celery.s(source, count) for count, source in enumerate(all_rss_sources))(save_fd.s(kwargs={'ner_types': "hello"}))
    except:
        pass
    #print rss_sources

    #if rss_sources == rss_sources_2:
    #    print "equal"
    #else:
    #    print "unequal"
    #print rss_sources_2    
    #Or this also works, but preferably use the above
    #print getMetric("articurate.fd.fd.getRSSSources_tech")
    logger.info("Num RSS sources "+str(len(rss_sources)))
    print "Num RSS sources = ", len(rss_sources)

@celery.task
def parse_source_celery(source, sourceCount):
    try:
        d = feedparser.parse(source.xmlUrl) #parse from rss source
        logger.info("Fetched "+str(source.xmlUrl))
        return jsonpickle.encode(d)
    except UnicodeEncodeError:
        logger.info("UnicodeUnicodeError while fetching "+str(source.xmlUrl))
        return 'False'
    except AttributeError:
        logger.info("AttributeError while fetching "+str(source.xmlUrl))        
        return 'False'
    except RuntimeError:
        logger.info("RuntimeError while fetching "+str(source.xmlUrl))        
        return 'False'        



@celery.task
def save_fd(results, **kwargs):
    print "Hello from save_fd"
    #Get the datapack
    datapack = getMetricByKey("articurate.fd.celery_tasks.saveDataToRedis", "tech")

    stringID = datapack['stringID']
    generated_on = datapack['generated_on']
    interval = datapack['interval']

    # create an xml tree
    root = Element('opml')
    root.set('version','1.0')
    root.append(Comment('Generated by FeedDigger'))
    
    head = SubElement(root, 'head')

    title = SubElement(head, 'title')
    title.text = 'grep last '+str(interval)+' minutes '+stringID
    
    dc = SubElement(head, 'dateCreated')
    dc.text = str(generated_on)

    dm = SubElement(head,'dateModified')
    dm.text = str(generated_on)
    
    body = SubElement(root, 'body')

    logger.info("Starting "+str(generated_on))

    num_added = 0

    # get entries from each of the rss sources
    for source_data in results:
        try:
            #print num_added
            num_added = num_added+1
            source_data_decoded = jsonpickle.decode(source_data)
            #d = feedparser.parse(source.xmlUrl) # parse from rss source
            try:
                d = source_data_decoded               
                logger.info("Parsed "+d.feed.title)  
                for entry in d.entries:
                    if (datetime(*entry.updated_parsed[:6]) > (generated_on-timedelta(minutes = interval))) and (datetime(*entry.updated_parsed[:6]) < (generated_on)):
                        # entry lies in required range
                        logger.info("Found "+entry.title+" at "+d.feed.title)

                        num_added = num_added + 1
                        print num_added

                        outline = getEntryContent(entry)

                        # create xml entry
                        item = SubElement(body, 'item')

                        # create xml title
                        title = SubElement(item, 'title')
                        title.text = outline.title

                        # create xml feed title
                        feedTitle = SubElement(item, 'feed_title')
                        feedTitle.text = d.feed.title

                        # create xml link
                        link = SubElement(item, 'link')
                        link.text = outline.link

                        # create xml author
                        author = SubElement(item, 'author')
                        author.text = outline.author

                        # create xml content
                        content = SubElement(item, 'content')
                        content.text = outline.content

                        # create xml timestamp
                        UpdatedAt = SubElement(item, 'updated_at')
                        UpdatedAt.text = str(datetime.fromtimestamp(mktime(outline.updatedAt)))
                        
                        

            except TypeError:
                    pass   
        except UnicodeEncodeError:
                pass
        except IndexError: #Verify
                pass
        except AttributeError:
                pass
        except ValueError:
                #Seems to be a ValueError that JSON could not be decoded in source_data decode, check later
                pass

    # output data to file
    #fout = open('../feeddumps/'+stringID+".opml",'w')
    #fout.write((prettify(root)).encode('utf-8'))
    #fout.close()
    #putCloud("river",stringID+".opml")
    #os.remove(stringID+".opml")


    # output dig to redis server
    storeDeltaDump(stringID,(prettify(root)).encode('utf-8'))

    # update dump key cache with latest entry
    updateDumpKeyCache(stringID)


    logger.info("Ended "+str(datetime.utcnow()))
